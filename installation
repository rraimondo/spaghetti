#Create a new VM on vSphere 7 using Rocky 9 OS minimal installation to create a rocky9 Template

# ip a
#Install updates
dnf makecache --refresh
dnf install epel-release -y
dnf repolist
dnf check-update
dnf clean all
dnf update -y

#Disable firewall
systemctl stop firewalld
systemctl disable firewalld

#Disable SELINUX
vi /etc/selinux/config
SELINUX=disabled
setenforce 0

#Install some packages
dnf -y install nano net-tools curl git wget gnupg vim yum-utils perl gcc make kernel-headers kernel-devel

#Install vmware-tools version
dnf install open-vm-tools
vmware-toolbox-cmd -v
12.0.5.35655 (build-19716617)

#Show DNS
cat /etc/resolv.conf
# Generated by NetworkManager
search lan
nameserver 192.168.1.1

reboot now

#Disable IPv6
vi /etc/default/grub 

GRUB_CMDLINE_LINUX="ipv6.disable=1 resume=/dev/mapper/cs-swap rd.lvm.lv=cs/root rd.lvm.lv=cs/swap rhgb quiet"

#Generate a new grub
grub2-mkconfig  -o /boot/efi/EFI/rocky/grub.cfg 
shutdown -r now

#Enable SSH root login
vi /etc/ssh/sshd_config
PermitRootLogin yes
PasswordAuthentication yes
PubkeyAuthentication yes

systemctl restart sshd

cat /etc/ssh/sshd_config | grep PermitRootLogin
PermitRootLogin yes
# the setting of "PermitRootLogin without-password".
Create a template rocky9

#Clone a new VM from this template name bastion
#Change hostname

hostnamectl set-hostname bastion
cat /etc/hostname 
bastion

vi /etc/hosts
192.168.1.110 bastion

hostname -f
bastion

systemctl restart systemd-hostnamed
shutdown -r now
hostnamectl

#Set Fixed IP Address

nmcli with modify 'ens192' ifname ens192 ipv4.method manual ipv4.addresses 192.168.1.110/24 gw4 192.168.1.1.1
nmcli with modify 'ens192' ipv4.dns 192.168.1.1

reboot now

#Create and install SSH Keys to ESXi nodes
ssh -V
OpenSSH_8.7p1, OpenSSL 3.0.1 14 Dec 2021

ssh-keygen -t rsa -N ''

Your identification has been saved in /root/.ssh/id_rsa.
Your public key has been saved in /root/.ssh/id_rsa.pub.

#Enable SSH on both ESXi nodes

Copy SSH Key to ESXi1 host
ssh-copy-id root@192.168.1.101
cat /root/.ssh/id_rsa| ssh root@192.168.1.101 'cat >> /etc/ssh/keys-root/authorized_keys'
cat /root/.ssh/id_rsa.pub| ssh root@192.168.1.101 'cat >> /etc/ssh/keys-root/authorized_keys'

ssh 192.168.1.101

#Copy SSH Key to ESXi2 host
ssh-copy-id root@192.168.1.102
cat /root/.ssh/id_rsa| ssh root@192.168.1.102 'cat >> /etc/ssh/keys-root/authorized_keys'
cat /root/.ssh/id_rsa.pub| ssh root@192.168.1.102 'cat >> /etc/ssh/keys-root/authorized_keys'

ssh 192.168.1.102
ssh 192.168.1.101 vim-cmd vmsvc/getallvms

#Install terraform on Rocky9
yum-config-manager --add-repo https://rpm.releases.hashicorp.com/RHEL/hashicorp.repo
yum -y install terraform
terraform version
Terraform v1.4.5
on linux_amd64

mkdir terraform
cd terraform/
mkdir modules

#Create the file credentials.tf
variable "vCenter_user" {
  description = "Username to connect to vCenter Server"
  type        = string
  default     = "administrator@vsphere.local"
}

variable "vCenter_password" {
  description = "Password to connect to vCenter Server"
  type        = string
  default     = "Admin123!"
}

variable "vCenter_server" {
  description = "IP or DNS name to connect to vCenter server"
  type        = string
  default     = "192.168.1.103"
}


#Create the file provider.tf

terraform {
  required_providers {
    vsphere = {
      source = "hashicorp/vsphere"
    }
  }
}

provider "vsphere" {
  user                 = var.vCenter_user
  password             = var.vCenter_password
  vsphere_server       = var.vCenter_server
  allow_unverified_ssl = true
}

#Create the file main.tf
variable "vminfo" {
  type = map(object({
    os     = string
    vmname = string
    cpu    = string
    memory = string
  }))
  default = {
    "server01" = {
      os     = "linux"
      vmname = "master1"
      cpu    = "2"
      memory = "4096"
    }
  "server02" = {
      os     = "windows"
      vmname = "dc"
      cpu    = "2"
      memory = "8192"
    }
  }
}
module "virtual_machine" {
  source     = "./modules/vsphere_vm"
  for_each   = var.vminfo
  datacenter = "Datacenter"
  cluster    = "Cluster"
  network    = "DSwitch-VM Network"
  datastore  = "nas"
  os         = each.value.os
  vmname     = each.value.vmname
  cpu        = each.value.cpu
  memory     = each.value.memory
}


cd modules/

#Create the file main.tf

data "vsphere_datacenter" "datacenter" {
  name = var.datacenter
}
data "vsphere_compute_cluster" "cluster" {
  name          = var.cluster
  datacenter_id = data.vsphere_datacenter.datacenter.id
}
data "vsphere_network" "network" {
  name          = var.network
  datacenter_id = data.vsphere_datacenter.datacenter.id
}
data "vsphere_datastore" "datastore" {
  name          = var.datastore
  datacenter_id = data.vsphere_datacenter.datacenter.id
}

data "vsphere_virtual_machine" "template1" {
name = "rocky9"
datacenter_id = data.vsphere_datacenter.datacenter.id
}

#data "vsphere_virtual_machine" "template2" {
#name = "win2022"
#datacenter_id = data.vsphere_datacenter.datacenter.id
#}

resource "vsphere_virtual_machine" "linuxvm1" {
#kubernetes
  lifecycle {
  prevent_destroy = false
 }
  count            = var.os == "linux" ? 1 : 0
  name             = "master1"
  resource_pool_id = data.vsphere_compute_cluster.cluster.resource_pool_id
  datastore_id     = data.vsphere_datastore.datastore.id
  num_cpus         = "2"
  memory           = "4096"
  guest_id         = data.vsphere_virtual_machine.template1.guest_id
  scsi_type        = data.vsphere_virtual_machine.template1.scsi_type
  folder           = "kubernetes"
  firmware         = "efi"
  network_interface {
    network_id   = data.vsphere_network.network.id
    adapter_type = data.vsphere_virtual_machine.template1.network_interface_types[0]
  }
  disk {
    label = "${var.vmname}-disk0"
    size             = "30"
#    size  = data.vsphere_virtual_machine.template1.disks.0.size
    thin_provisioned = data.vsphere_virtual_machine.template1.disks.0.thin_provisioned
  }
  clone {
    template_uuid = data.vsphere_virtual_machine.template1.id
    timeout = 60
    customize {
    timeout = 60
      linux_options {
        host_name = "master1"
        domain    = ""
      }
      network_interface {
            ipv4_address = "192.168.1.201"
            ipv4_netmask = "24"
            dns_server_list = ["192.168.1.1"]      
        }
      ipv4_gateway = "192.168.1.1"
     }
  }
}
  
resource "vsphere_virtual_machine" "linuxvm2" {
#kubernetes
  lifecycle {
  prevent_destroy = false
 }
  count            = var.os == "linux" ? 1 : 0
  name             = "worker1"
  resource_pool_id = data.vsphere_compute_cluster.cluster.resource_pool_id
  datastore_id     = data.vsphere_datastore.datastore.id
  num_cpus         = "2"
  memory           = "4096"
  guest_id         = data.vsphere_virtual_machine.template1.guest_id
  scsi_type        = data.vsphere_virtual_machine.template1.scsi_type
  folder           = "kubernetes"
  firmware         = "efi"
  network_interface {
    network_id   = data.vsphere_network.network.id
    adapter_type = data.vsphere_virtual_machine.template1.network_interface_types[0]
  }
  disk {
    label = "${var.vmname}-disk0"
    size             = "30"
#    size  = data.vsphere_virtual_machine.template1.disks.0.size
    thin_provisioned = data.vsphere_virtual_machine.template1.disks.0.thin_provisioned
  }
  clone {
    template_uuid = data.vsphere_virtual_machine.template1.id
    timeout = 60
    customize {
    timeout = 60
      linux_options {
        host_name = "worker1"
        domain    = ""
      }
      network_interface {
            ipv4_address = "192.168.1.202"
            ipv4_netmask = "24"
            dns_server_list = ["192.168.1.1"]      
        }
      ipv4_gateway = "192.168.1.1"
     }
  }
  }

resource "vsphere_virtual_machine" "linuxvm3" {
#kubernetes
  lifecycle {
  prevent_destroy = false
 }
  count            = var.os == "linux" ? 1 : 0
  name             = "worker2"
  resource_pool_id = data.vsphere_compute_cluster.cluster.resource_pool_id
  datastore_id     = data.vsphere_datastore.datastore.id
  num_cpus         = "2"
  memory           = "4096"
  guest_id         = data.vsphere_virtual_machine.template1.guest_id
  scsi_type        = data.vsphere_virtual_machine.template1.scsi_type
  folder           = "kubernetes"
  firmware         = "efi"
  network_interface {
    network_id   = data.vsphere_network.network.id
    adapter_type = data.vsphere_virtual_machine.template1.network_interface_types[0]
  }
  disk {
    label = "${var.vmname}-disk0"
    size             = "30"
#    size  = data.vsphere_virtual_machine.template1.disks.0.size
    thin_provisioned = data.vsphere_virtual_machine.template1.disks.0.thin_provisioned
  }
  clone {
    template_uuid = data.vsphere_virtual_machine.template1.id
    timeout = 60
    customize {
    timeout = 60
      linux_options {
        host_name = "worker2"
        domain    = ""
      }
      network_interface {
            ipv4_address = "192.168.1.203"
            ipv4_netmask = "24"
            dns_server_list = ["192.168.1.1"]      
        }
      ipv4_gateway = "192.168.1.1"
     }
    }
  }
  
#Create the file variables.tf
variable "datacenter" {
  description = "vCenter Datacenter name"
  type        = string
 # default     = "Datacenter"
}

variable "cluster" {
  description = "vCenter Server Cluster name"
  type        = string
 # default     = "Cluster"
}

variable "network" {
  description = "vCenter Network name"
  type        = string
 # default     = "DSwitch-VM Network"
}

variable "datastore" {
  description = "vCenter Datastore name"
  type        = string
 # default     = "nas"
}

variable "template1" {
  description = "vCenter template name"
  type        = string
  default     = "rocky9"
}

variable "template2" {
  description = "vCenter template name"
  type        = string
  default     = "win2022"
}

variable "os" {
  description = "Operating system of new VM"
  type        = string
  #default     = "linux"
}

variable "vmname" {
  description = "Provide New virtual machine name"
  type        = string
  #default     = "demo"
}

variable "cpu" {
  description = "Provide New virtual machine cpu count"
  type        = string
  #default     = "1"
}

variable "memory" {
  description = "Provide New virtual machine memory"
  type        = string
  #default     = "1024"
}

variable "enable_disk_uuid" {
  description = "Expose the UUIDs of attached virtual disks to the virtual machine, allowing access to them in the guest. Default: Inherited from cloned template"
  type        = bool
  default     = true
}

variable "efi_secure_boot" {
  description = "Enables EFI secure boot. Can be only be true when firmware is EFI. Default: Inherited from cloned template"
  default     = false
}

ls
credentials.tf  main.tf  modules  provider.tf

terraform init
terraform validate
terraform plan
terraform apply --auto-approve

#to destroy
terraform destroy -auto-approve

#After machines get created you'll have 1 master and 2 workers nodes on vSphere 7

#Install Kubernetes using Kubespray on rocky9

#On the bastion machine

vi /etc/hosts

127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6

192.168.1.110 bastion
192.168.1.201 master1
192.168.1.202 worker1
192.168.1.203 worker2

[root@bastion ~]# cat /etc/resolv.conf
# Generated by NetworkManager
nameserver 192.168.1.1
nameserver 8.8.8.8

modprobe overlay
modprobe br_netfilter

tee /etc/sysctl.d/kubernetes.conf<<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF

sysctl --system

sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
swapoff -a

from the bastion machine copy ssh keys to all nodes

ssh-copy-id -i ~/.ssh/id_rsa root@192.168.1.201
ssh-copy-id -i ~/.ssh/id_rsa root@192.168.1.202
ssh-copy-id -i ~/.ssh/id_rsa root@192.168.1.203

ssh 192.168.1.201


Install Kubespray on ansible machine

dnf config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
dnf makecache
dnf install -y containerd.io

git clone https://github.com/kubernetes-sigs/kubespray.git
cd kubespray/

Install Ansible and pyvmomi module

yum install epel-release
yum install python-pip
pip --version
pip 21.2.3 from /usr/lib/python3.9/site-packages/pip (python 3.9)

pip3 install --upgrade pip
pip3 install wheel
pip3 install setuptools-rust
pip3 install ansible

ansible --version
ansible [core 2.14.4]

pip3 install --upgrade pyvmomi
ansible-galaxy collection install community.vmware
pip3 install --upgrade git+https://github.com/vmware/vsphere-automation-sdk-python.git
pip3 install pywinrm
pip3 install jmespath
python3 -m pip show requests
dnf install ansible

pip3 install -r requirements.txt

cp -rfp inventory/sample inventory/mycluster

declare -a IPS=(192.168.1.201 192.168.1.202 192.168.1.203)
CONFIG_FILE=inventory/mycluster/hosts.yaml python3 contrib/inventory_builder/inventory.py ${IPS[@]}

vi inventory/mycluster/hosts.yaml

all:
  hosts:
    master1:
      ansible_host: 192.168.1.201
      ip: 192.168.1.201
      access_ip: 192.168.1.201
    worker1:
      ansible_host: 192.168.1.202
      ip: 192.168.1.202
      access_ip: 192.168.1.202
    worker2:
      ansible_host: 192.168.1.203
      ip: 192.168.1.203
      access_ip: 192.168.1.203
  children:
    kube_control_plane:
      hosts:
        master1:
      kube_node:
      hosts:
        master1:
        worker1:
        worker2:
    etcd:
      hosts:
        master1:
        worker1:
        worker2:
    k8s_cluster:
      children:
        kube_control_plane:
        kube_node:
    calico_rr:
      hosts: {}

[root@bastion kubespray]# vi inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml

#IP Addresses Range 
#10.0.0.0/8 IP addresses: 10.0.0.0 – 10.255.255.255
#172.16.0.0/12 IP addresses: 172.16.0.0 – 172.31.255.255
#192.168.0.0/16 IP addresses: 192.168.0.0 – 192.168.255.255

# These values are valid IP addresses or network ranges
#'192.168.0.1'       -> 192.168.0.1
#'192.168.32.0/24'   -> 192.168.32.0/24
#'fe80::100/10'      -> fe80::100/10
#45443646733         -> ::a:94a7:50d
#'523454/24'         -> 0.7.252.190/24

#If using Calico, the recommended Pod Network is: 192.168.0.0/16
#For Flannel, it is recommended you set Pod network to 10.244.0.0/16

kube_network_plugin: flannel
kube_network_plugin_multus: true
kube_service_addresses: 10.233.0.0/18
kube_pods_subnet: 10.233.64.0/18
kube_network_node_prefix: 24
kube_apiserver_port: 6443
kube_proxy_mode: iptables
kube_proxy_strict_arp: true
cluster_name: cluster.local
dns_mode: coredns
enable_nodelocaldns: true
nodelocaldns_ip: 192.168.1.1
deploy_netchecker: true
container_manager: containerd
kubeconfig_localhost: true

[root@awx kubespray]# vi inventory/mycluster/group_vars/all/all.yml

cloud_provider:  "external"
external_cloud_provider: "vsphere"

vi inventory/mycluster/group_vars/all/vsphere.yml

## Values for the external vSphere Cloud Provider
external_vsphere_vcenter_ip: "192.168.1.103"
external_vsphere_vcenter_port: "443"
external_vsphere_insecure: "true"
external_vsphere_user: "administrator@vsphere.local" # Can also be set via the `VSPHERE_USER` environment variable
external_vsphere_password: "Admin123!" # Can also be set via the `VSPHERE_PASSWORD` environment variable
external_vsphere_datacenter: "Datacenter"
external_vsphere_kubernetes_cluster_id: "kubernetes-cluster"

vi inventory/mycluster/group_vars/k8s_cluster/addons.yml

dashboard_enabled: true
helm_enabled: true
metrics_server_enabled: true
ingress_nginx_enabled: true
ingress_nginx_host_network: false
ingress_publish_status_address: ""

#After nodes are configured then launch next playbook to create the kubernetes cluster:

ansible all -i inventory/mycluster/hosts.yaml -m shell -a "sudo systemctl stop firewalld && sudo systemctl disable firewalld"

ansible all -i inventory/mycluster/hosts.yaml -m shell -a "echo 'net.ipv4.ip_forward=1' | sudo tee -a /etc/sysctl.conf"

ansible all -i inventory/mycluster/hosts.yaml -m shell -a "sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab && sudo swapoff -a"

ansible all -i inventory/mycluster/hosts.yaml -m shell -a "echo nameserver 192.168.1.1 | tee -a /etc/resolv.conf"
ansible all -i inventory/mycluster/hosts.yaml -m shell -a "echo nameserver 8.8.8.8 | tee -a /etc/resolv.conf"

#Run the playbook:
ansible-playbook -i inventory/mycluster/hosts.yaml --become --become-user=root cluster.yml

on master node:

[root@master1 ~]# kubectl version --client --output=yaml
clientVersion:
  buildDate: "2023-03-15T13:40:17Z"
  compiler: gc
  gitCommit: 9e644106593f3f4aa98f8a84b23db5fa378900bd
  gitTreeState: clean
  gitVersion: v1.26.3
  goVersion: go1.19.7
  major: "1"
  minor: "26"
  platform: linux/amd64
kustomizeVersion: v4.5.7

[root@master1 ~]# kubectl cluster-info
Kubernetes control plane is running at https://127.0.0.1:443

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.

Access Kubernetes cluster
Login to first master node, switch to root user, run kubectl commands from there,

On master node:

[root@master1 ~]# kubectl get nodes
NAME      STATUS   ROLES           AGE   VERSION
master1   Ready    control-plane   91m   v1.26.3
worker1   Ready    <none>          89m   v1.26.3
worker2   Ready    <none>          89m   v1.26.3

[root@master ~]# kubectl get pods -A

#Deploy nginx based deployment and expose it as nodeport

[root@master1 ~]# kubectl create deployment nginx-kubespray --image=nginx --replicas=2
deployment.apps/nginx-kubespray created

[root@master1 ~]# kubectl expose deployment nginx-kubespray --type NodePort --port=80
service/nginx-kubespray exposed

[root@master1 ~]# kubectl get  deployments.apps
NAME                READY   UP-TO-DATE   AVAILABLE   AGE
netchecker-server   1/1     1            1           89m
nginx-kubespray     2/2     2            2           54s

[root@master1 ~]# kubectl get pods
NAME                                 READY   STATUS    RESTARTS      AGE
netchecker-agent-24wz6               1/1     Running   0             89m
netchecker-agent-47z8g               1/1     Running   0             89m
netchecker-agent-hostnet-8w766       1/1     Running   0             89m
netchecker-agent-hostnet-b4hgm       1/1     Running   0             89m
netchecker-agent-hostnet-nc589       1/1     Running   0             89m
netchecker-agent-tjgsf               1/1     Running   0             89m
netchecker-server-6cf9dcfdc5-dpnxf   2/2     Running   1 (89m ago)   89m
nginx-kubespray-866c876988-9qkkt     1/1     Running   0             58s
nginx-kubespray-866c876988-d7wvf     1/1     Running   0             58s

[root@master1 ~]# kubectl get svc nginx-kubespray
NAME              TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
nginx-kubespray   NodePort   10.233.21.126   <none>        80:30605/TCP   64s


On worker node 1
[root@master1 ~]# curl 192.168.1.201:30605
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>


http://192.168.1.201:30605/

#To delete:
kubectl get service 
kubectl delete deployment nginx-kubespray
kubectl delete service nginx-kubespray


#Install GUI on master node
[root@master1 ~]# dnf update
[root@master1 ~]# dnf grouplist
[root@master1 ~]# dnf groupinstall "Server with GUI"
[root@master1 ~]# systemctl set-default graphical.target
[root@master1 ~]# systemctl isolate graphical

# Install Kubernetes Dashboard (GUI) on master node

[root@master1 ~]# vi kubernetes-dashboard-ingress.yaml

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: k8s-dashboard-ingress
  annotations:
    nginx.ingress.kubernetes.io/secure-backends: "true"
    kubernetes.io/ingress.class: "nginx"
    nginx.ingress.kubernetes.io/ssl-passthrough: "true"
    nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
spec:
  tls:
  - hosts:
      - "kubernetes.dashboard"
        #    secretName: kubernetes-dashboard-secret
  rules:
  - host: dashboard.kubernetes
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: kubernetes-dashboard
            port:
              number: 443


[root@master1 ~]# kubectl create -f kubernetes-dashboard-ingress.yaml -n kube-system

[root@master1 ~]# vi kubernetes-dashboard-admin-user.yaml

apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kube-system

[root@master1 ~]# kubectl create -f kubernetes-dashboard-admin-user.yaml

[root@master1 ~]# wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml -O kubernetes-dashboard.yaml

[root@master1 ~]# vi kubernetes-dashboard.yaml

          args:
            - --auto-generate-certificates
            - --namespace=kubernetes-dashboard
            - --enable-skip-login
            - --disable-settings-authorizer
            - --enable-insecure-login
            - --insecure-bind-address=0.0.0.0

[root@master1 ~]# kubectl apply -f  kubernetes-dashboard.yaml
[root@master1 ~]# kubectl describe svc kubernetes-dashboard -n kubernetes-dashboard
Name:              kubernetes-dashboard
Namespace:         kubernetes-dashboard
Labels:            k8s-app=kubernetes-dashboard
Annotations:       <none>
Selector:          k8s-app=kubernetes-dashboard
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.233.43.220
IPs:               10.233.43.220
Port:              <unset>  443/TCP
TargetPort:        8443/TCP
Endpoints:         10.233.65.36:8443
Session Affinity:  None
Events:            <none>

[root@master1 ~]# kubectl -n kube-system get all

[root@master1 ~]# kubectl create token admin-user -n kube-system
[root@master1 ~]# kubectl proxy
[root@master1 ~]# firefox http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/

#Insert Token and login

Adding nodes

You may want to add worker, control plane or etcd nodes to your existing cluster. This can be done by re-running the cluster.yml playbook, or you can target the bare minimum needed to get kubelet installed on the worker and talking to your control planes. This is especially helpful when doing something like autoscaling your clusters.

Add the new worker node to your inventory in the appropriate group (or utilize a dynamic inventory).

Run the ansible-playbook command, substituting cluster.yml for scale.yml:

ansible-playbook -i inventory/mycluster/hosts.yml scale.yml -b -v \
  --private-key=~/.ssh/private_key

Remove nodes

You may want to remove control plane, worker, or etcd nodes from your existing cluster. This can be done by re-running the remove-node.yml playbook. First, all specified nodes will be drained, then stop some kubernetes services and delete some certificates, and finally execute the kubectl command to delete these nodes. This can be combined with the add node function. This is generally helpful when doing something like autoscaling your clusters. Of course, if a node is not working, you can remove the node and install it again.

Use --extra-vars "node=<nodename>,<nodename2>" to select the node(s) you want to delete.

ansible-playbook -i inventory/mycluster/hosts.yml remove-node.yml -b -v \
--private-key=~/.ssh/private_key \
--extra-vars "node=nodename,nodename2"

If a node is completely unreachable by ssh, add --extra-vars reset_nodes=false to skip the node reset step. If one node is unavailable, but others you wish to remove are able to connect via SSH, you could set reset_nodes=false as a host var in inventory.

#Install docker on master node

[root@master1 ~]# dnf config-manager --add-repo=https://download.docker.com/linux/centos/docker-ce.repo
[root@master1 ~]# dnf install -y docker-ce
[root@master1 ~]# docker version
[root@master1 ~]# curl -L "https://github.com/docker/compose/releases/download/v2.12.2/docker-compose-$(uname -s)-$(uname -m)"  -o /usr/local/bin/docker-compose
[root@master1 ~]# chmod +x /usr/local/bin/docker-compose
[root@master1 ~]# mv /usr/local/bin/docker-compose /usr/bin/docker-compose
[root@master1 ~]# chmod +x /usr/bin/docker-compose
[root@master1 ~]# dnf install docker-compose-plugin
[root@master1 ~]# docker compose version
[root@master1 ~]# systemctl start docker
[root@master1 ~]# systemctl enable docker
[root@master1 ~]# systemctl status docker
[root@master1 ~]# usermod -aG docker $USER
[root@master1 ~]# newgrp docker
[root@master1 ~]# docker run hello-world

#Install Octopus

[root@master1 ~]# docker pull octopusdeploy/octopusdeploy
[root@master1 ~]# vi docker-compose.yml

version: '3'
services:
   db:
    image: ${SQL_IMAGE}
    environment:
      SA_PASSWORD: ${SA_PASSWORD}
      ACCEPT_EULA: ${ACCEPT_EULA}
    ports:
      - 1401:1433
    healthcheck:
      test: [ "CMD", "/opt/mssql-tools/bin/sqlcmd", "-U", "sa", "-P", "${SA_PASSWORD}", "-Q", "select 1"]
      interval: 10s
      retries: 10
    volumes:
      - sqlvolume:/var/opt/mssql
   octopus-server:
    image: octopusdeploy/octopusdeploy:${OCTOPUS_SERVER_TAG}
    privileged: ${PRIVILEGED}
    user: ${USER}
    environment:
      ACCEPT_EULA: ${ACCEPT_OCTOPUS_EULA}
      OCTOPUS_SERVER_NODE_NAME: ${OCTOPUS_SERVER_NODE_NAME}
      DB_CONNECTION_STRING: ${DB_CONNECTION_STRING}
      ADMIN_USERNAME: ${ADMIN_USERNAME}
      ADMIN_PASSWORD: ${ADMIN_PASSWORD}
      ADMIN_EMAIL: ${ADMIN_EMAIL}
      OCTOPUS_SERVER_BASE64_LICENSE: ${OCTOPUS_SERVER_BASE64_LICENSE}
      MASTER_KEY: ${MASTER_KEY}
      ADMIN_API_KEY: ${ADMIN_API_KEY}
      DISABLE_DIND: ${DISABLE_DIND}
    ports:
      - 8080:8080
      - 11111:10943
    depends_on:
      - db
    volumes:
      - repository:/repository
      - artifacts:/artifacts
      - taskLogs:/taskLogs
      - cache:/cache
      - import:/import
volumes:
  repository:
  artifacts:
  taskLogs:
  cache:
  import:
  sqlvolume:


[root@master1 ~]# vi .env file

# Define the password for the SQL database. This also must be set in the DB_CONNECTION_STRING value.
SA_PASSWORD=Admin123!

# Tag for the Octopus Deploy Server image. Use "latest" to pull the latest image or specify a specific tag
OCTOPUS_SERVER_TAG=latest

# Sql Server image. Set this variable to the version you wish to use. Default is to use the latest.
SQL_IMAGE=mcr.microsoft.com/mssql/server

# The default created user username for login to the Octopus Server
ADMIN_USERNAME=roy

# It is highly recommended this value is changed as it's the default user password for login to the Octopus Server
ADMIN_PASSWORD=Admin123!

# Email associated with the default created user. If empty will default to octopus@example.local
ADMIN_EMAIL=

# Accept the Microsoft Sql Server Eula found here: https://hub.docker.com/r/microsoft/mssql-server-windows-express/
ACCEPT_EULA=Y

# Use of this Image means you must accept the Octopus Deploy Eula found here: https://octopus.com/company/legal
ACCEPT_OCTOPUS_EULA=Y

# Unique Server Node Name - If left empty will default to the machine Name
OCTOPUS_SERVER_NODE_NAME=

# Database Connection String. If using database in sql server container, it is highly recommended to change the password.
DB_CONNECTION_STRING=Server=db,1433;Database=OctopusDeploy;User=sa;Password=Admin123!

# Your License key for Octopus Deploy. If left empty, it will try and create a free license key for you
OCTOPUS_SERVER_BASE64_LICENSE=

# Octopus Deploy uses a master key for encryption of your databse. If you're using an external database that's already been setup for Octopus Deploy, you can supply the master key to use it.
# If left blank, a new master key will be generated with the database creation.
# Create a new master key with the command: openssl rand 16 | base64
MASTER_KEY=

# The API Key to set for the administrator. If this is set and no password is provided then a service account user will be created. If this is set and a password is also set then a standard user will be created.
ADMIN_API_KEY=

# Docker-In-Docker is used to support worker container images. It can be disabled by setting DISABLE_DIND to Y.
# The container only requires the privileged setting if DISABLE_DIND is set to N.
DISABLE_DIND=Y
PRIVILEGED=false

# Octopus can be run either as the user root or as octopus.
USER=octopus

[root@master1 ~]# docker-compose up

http://192.168.1.201:8080/app#/users/sign-in

